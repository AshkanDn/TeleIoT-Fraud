{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a85761",
   "metadata": {},
   "source": [
    "### Section 1. Loading Essential Libraries & Initialise Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2ae46-afbd-42c4-8f38-3cdcb83fa284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Custom module imports\n",
    "from src.eda import EDA  # Load EDA Class\n",
    "from src.enrich import *  # Import enrichment functions\n",
    "\n",
    "\n",
    "def initialise_project():\n",
    "    \"\"\"\n",
    "    Initialize the project:: setting the working directory, loading the configuration,\n",
    "    and configuring pandas settings.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configuration loaded from the YAML file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Change to the project directory\n",
    "        PROJECT_DIR = './dashtban/TeleIoT-Fraud/'\n",
    "        os.chdir(PROJECT_DIR)\n",
    "        print(f\"Working directory set to: {os.getcwd()}\")\n",
    "\n",
    "        # Load configuration\n",
    "        CONFIG_PATH = 'config.yml'\n",
    "        if not os.path.exists(CONFIG_PATH):\n",
    "            raise FileNotFoundError(f\"Configuration file '{CONFIG_PATH}' not found in the project directory.\")\n",
    "\n",
    "        with open(CONFIG_PATH, 'r') as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Configure pandas settings\n",
    "        pd.set_option('mode.use_inf_as_na', True)\n",
    "        print(\"Pandas configuration set: mode.use_inf_as_na = True\")\n",
    "\n",
    "        print(\"Project successfully initialized.\")\n",
    "        return config\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "\n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"Error loading configuration file: {e}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during project initialization: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialise the project and load configuration\n",
    "config = initialise_project()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c94d06",
   "metadata": {},
   "source": [
    "### Section 2. Processing Scanned Time-Series Files from R Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd8b00-2df4-4186-b0c2-6ba6d12281f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wq/qwr2827d25g4h82c4hdh7mqw0000gn/T/ipykernel_98726/670893928.py:2: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(config['data']['rdata_scanned'])\n"
     ]
    }
   ],
   "source": [
    "# Load the scanned dataset (R data processed by R scripts)\n",
    "# Use the 'rdata_scanned' key from the config to load the dataset\n",
    "df = pd.read_csv(config['data']['rdata_scanned'])\n",
    "\n",
    "# Clean column names by removing the 'positions.' prefix for easier access in the next steps\n",
    "df.columns = [col.replace('positions.', '') for col in df.columns]\n",
    "\n",
    "# Initialise the EDA class with the loaded DataFrame\n",
    "eda = EDA(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766cf98f",
   "metadata": {},
   "source": [
    "### Stage 3 : Investigating Data & Generating Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd058b7-687e-4fb3-bcd3-65f2970a3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Characteristics Before Data Cleansing\n",
    "eda.column_statistics(df, config['path']['report'] + 'column_statistics_original_data.html')\n",
    "\n",
    "# Generate Summary Statistics for Numerical Values\n",
    "eda.summary_html(df, config['path']['report'] + 'summary_statistics_numerical_values.html')\n",
    "\n",
    "# Generate Geospatial Maps Based on Data\n",
    "eda.generate_maps(df, config['reports']['maps'])\n",
    "\n",
    "# Select Random Rows Without Missing Values for Inspection\n",
    "eda.select_random_rows_without_missing_values(df).to_csv(config['path']['report'] + 'temp.csv', index=False)\n",
    "\n",
    "# Select Rows with Missing Values and Save to HTML for Review\n",
    "eda.select_rows_with_missing_values(df, config['path']['report'] + 'rows_with_missing_values.html')\n",
    "\n",
    "# Generate Correlation Table for the Original Data\n",
    "eda.generate_correlation_table(df, config['path']['report'] + 'summary_numerical_original_cohort.html')\n",
    "\n",
    "# Generate a Correlation Heatmap for the Data\n",
    "eda.generate_correlation_heatmap(df, config['path']['report'] + 'df_original_cohort_corr.jpg')\n",
    "\n",
    "# Add Missing Indicator Column to the DataFrame (Threshold = 1% Missing)\n",
    "df = eda.add_missing_indicator_column(df, threshold=0.01, colname='missing')\n",
    "\n",
    "# Save the Refined DataFrame to Disk for Future Use\n",
    "df.to_pickle(config['path']['data'] + 'df_refined.pkl')\n",
    "\n",
    "# Reload the Refined DataFrame from the Pickle File\n",
    "df = pd.read_pickle(config['path']['data'] + 'df_refined.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c904c9b",
   "metadata": {},
   "source": [
    "### Stage 4 : Enriching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba850cb4-67c2-4752-b5ad-7c2d332e9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Enrichment Pipeline to the Scanned R-DataFrame\n",
    "df = enrichment_pipeline(df)\n",
    "\n",
    "# Save the Refined and Enriched DataFrame to a Pickle File for Future Use\n",
    "df.to_pickle(config['data']['refined_enriched'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e365e3",
   "metadata": {},
   "source": [
    "### Stage 5 : Investigating the Enriched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4544383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the enriched data from the pickle file\n",
    "df = pd.read_pickle(config['data']['refined_enriched'])\n",
    "\n",
    "# Initialize the EDA class with the enriched DataFrame\n",
    "eda = EDA(df)\n",
    "\n",
    "# Generate correlation heatmap for the entire DataFrame\n",
    "eda.generate_correlation_heatmap(df, config['path']['report'] + 'df_enriched_cohort_corr.jpg')\n",
    "\n",
    "# Select specific columns of interest for detailed correlation analysis\n",
    "selected_columns = [\n",
    "    'duration', 'total_distance', 'average_speed', 'duration_minutes', 'direct_distance', \n",
    "    'gforce', 'Signal_Strength', 'Wavelet_XYZ_1', 'Wavelet_XYZ_2', 'Wavelet_XYZ_3', \n",
    "    'Wavelet_LonLat_1', 'Wavelet_LonLat_2', 'Wavelet_LonLat_3', 'PCA_gforce_1', \n",
    "    \"PCA_speedMph_1\", 'Distinct_formOfWay_Count', 'incident'\n",
    "]\n",
    "\n",
    "# Generate correlation heatmap for the selected columns\n",
    "eda.generate_correlation_heatmap(df[selected_columns], config['path']['report'] + 'df_enriched_cohort_corr_selected.jpg')\n",
    "\n",
    "# Generate column statistics for the enriched data\n",
    "eda.column_statistics(df, config['path']['report'] + 'column_statistics_enriched_data.html')\n",
    "\n",
    "# Generate summary statistics for numerical values in the enriched data\n",
    "eda.summary_html(df, config['path']['report'] + 'summary_statistics_numerical_values_enriched.html')\n",
    "\n",
    "# Generate geospatial maps based on the enriched data\n",
    "eda.generate_maps(df, config['reports']['maps'])\n",
    "\n",
    "# Select and save random rows without missing values to CSV\n",
    "eda.select_random_rows_without_missing_values(df).to_csv(config['path']['report'] + 'temp_enriched.csv', index=False)\n",
    "\n",
    "# Select and save random rows with missing values to an HTML report\n",
    "eda.select_rows_with_missing_values(df, config['path']['report'] + 'rows_with_missing_values_enriched.html')\n",
    "\n",
    "# Generate correlation table for numerical features in the enriched data\n",
    "eda.generate_correlation_table(df, config['path']['report'] + 'summary_numerical_enriched_cohort.html')\n",
    "\n",
    "# Generate another correlation heatmap for the entire enriched cohort\n",
    "eda.generate_correlation_heatmap(df, config['path']['report'] + 'df_enriched_cohort_corr_final.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cab685",
   "metadata": {},
   "source": [
    "### Stage 6 : Algorithm Design, Modeling, & Pre-Evaludation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7031ce2f",
   "metadata": {},
   "source": [
    "<div>\n",
    "\n",
    "#### Algorithm 1: Incident Prediction and Risk Assessment\n",
    "\n",
    "1. **Distance Calculation**:  \n",
    "   - Compute the distance between all data points and the observed incidents.\n",
    "\n",
    "2. **Selection of Farthest Points**:  \n",
    "   - Identify the farthest data point from the incidents and select as many points as there are observed incidents to create a balanced dataset.\n",
    "\n",
    "3. **Model Training**:  \n",
    "   - Train a machine learning model to distinguish between incidents and non-incidents.\n",
    "\n",
    "4. **Probability Update**:  \n",
    "   - Use the trained model to predict probabilities. Update the index pool by assigning a label of \"incident\" if the probability is greater than 70%.\n",
    "\n",
    "5. **Iteration**:  \n",
    "   - With the updated labels, return to step 2. Continue the process until there are no more data points left to label.\n",
    "\n",
    "6. **Risk Assessment**:  \n",
    "   - Evaluate the highest-risk journeys by counting the number of predicted incidents per journey. Assign a severity index for each journey based on the number of predicted incidents.\n",
    "\n",
    "7. **Classifier Refinement**:  \n",
    "   - Train a second classifier using the updated labels and evaluate how well the true incidents can be detected.\n",
    "\n",
    "8. **Refinement of the Process**:  \n",
    "   - Based on the findings from step 7, refine the procedure and the model for improved incident detection and risk assessment.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f0d43-5203-4551-8ad3-b00ae0e250c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### Steps 1 and 2 of Algorithm 1\n",
    "1. **Distance Calculation**:  \n",
    "   - Compute the distance between all data points and the observed incidents.\n",
    "2. **Selection of Farthest Points**:  \n",
    "   - Identify the farthest data point from the incidents and select as many points as there are observed incidents to create a balanced dataset.\n",
    "\"\"\"\n",
    "\n",
    "def find_distant_points(df, label_column='incident', num_points=12000, sel_cols=None):\n",
    "    \"\"\"\n",
    "    Finds the data points with the highest distance between instances with label 0 (non-incidents) and label 1 (incidents).\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        label_column (str): The column name representing the label ('incident').\n",
    "        num_points (int): The number of data points to select with the highest distance.\n",
    "        sel_cols (list, optional): List of columns to consider for distance computation. Default is None, which uses all columns.\n",
    "        \n",
    "    Returns:\n",
    "        high_distance_points_df (DataFrame): DataFrame with an additional column indicating the highest distance points.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the DataFrame uses only numeric values for distance calculation\n",
    "        if sel_cols:\n",
    "            df = df[sel_cols]\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        # Drop rows with missing values\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Separate data points based on the label (incident: 1, non-incident: 0)\n",
    "        label_0 = df[df[label_column] == 0].values\n",
    "        label_1 = df[df[label_column] == 1].values\n",
    "        \n",
    "        # Check for the presence of both label 0 and label 1 data points\n",
    "        if len(label_0) == 0 or len(label_1) == 0:\n",
    "            raise ValueError(f\"No data points with label 0 or label 1 found.\")\n",
    "        \n",
    "        # Compute the pairwise Euclidean distance between points of label 0 and label 1\n",
    "        distances = np.sqrt(((label_0[:, np.newaxis] - label_1) ** 2).sum(axis=2))\n",
    "        \n",
    "        # Get indices of the top `num_points` data points with the highest distances\n",
    "        top_indices = np.unravel_index(np.argsort(distances.ravel())[-num_points:], distances.shape)\n",
    "        \n",
    "        # Create DataFrame with all original columns\n",
    "        high_distance_points_df = df.copy()\n",
    "        \n",
    "        # Add a column to indicate whether the data point is one of the highest distance points\n",
    "        high_distance_points_df['Is_Highest_Distance'] = 0\n",
    "        high_distance_points_df.iloc[top_indices[0], high_distance_points_df.columns.get_loc(label_column)] = 1\n",
    "        \n",
    "        return high_distance_points_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Select the same number of samples as observed incidents\n",
    "num_samples = df[df['incident'] == 1].shape[0]\n",
    "\n",
    "# Select features for computing similarity (use predefined columns)\n",
    "sel_cols = ['duration', 'total_distance', 'average_speed', 'duration_minutes', 'direct_distance', \n",
    "            'gforce', 'Signal_Strength', 'Wavelet_XYZ_1', 'Wavelet_XYZ_2', 'Wavelet_XYZ_3',\n",
    "            'Wavelet_LonLat_1', 'Wavelet_LonLat_2', 'Wavelet_LonLat_3', 'PCA_gforce_1', \n",
    "            'PCA_speedMph_1', 'Distinct_formOfWay_Count', 'incident']\n",
    "\n",
    "# Select top 1 row per UID (if grouping by 'UID' to ensure unique data points)\n",
    "df_1 = df.groupby('UID').head(1)\n",
    "\n",
    "# Sample a subset of rows to improve computational efficiency\n",
    "df_sample = df_1.sample(n=1000, random_state=42)\n",
    "\n",
    "# Call the function to get the distant points\n",
    "distant_df = find_distant_points(df_sample, label_column='incident', num_points=num_samples, sel_cols=sel_cols)\n",
    "\n",
    "# You can now work with `distant_df` for further analysis or modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07da49-38cd-4919-b4f7-b6ea5a8ec381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 3 of Algorithm 1: Model Training (LightGBM)\n",
    "#\n",
    "# Train a machine learning model to distinguish between incidents and non-incidents.\n",
    "# In this case, we will train a LightGBM model.\n",
    "#\n",
    "#\n",
    "\n",
    "def train_lightgbm_model(df, target_column='incident', test_size=0.2, random_state=42, params=None, num_round=100, plot_file='feature_importance.png'):\n",
    "    \"\"\"\n",
    "    Trains a LightGBM model to classify incidents (target_column = 1) vs non-incidents (target_column = 0).\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame containing features and the target column.\n",
    "        target_column (str): The column name representing the target variable (default is 'incident').\n",
    "        test_size (float): The fraction of the dataset to be used for testing (default is 0.2).\n",
    "        random_state (int): The seed for random number generation (default is 42).\n",
    "        params (dict, optional): Hyperparameters for the LightGBM model. If None, default parameters are used.\n",
    "        num_round (int): The number of boosting iterations (default is 100).\n",
    "        plot_file (str): The file name to save the feature importance plot (default is 'feature_importance.png').\n",
    "    \n",
    "    Returns:\n",
    "        bst (Booster): The trained LightGBM model.\n",
    "        filtered_df (DataFrame): The filtered DataFrame with predicted probabilities added.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter the DataFrame for the rows where the target_column equals 1 (incidents)\n",
    "        filtered_df = df[df[target_column] == 1].copy()\n",
    "\n",
    "        # Separate features (X) and target (y)\n",
    "        X = filtered_df.drop(target_column, axis=1)  # Features (all columns except target)\n",
    "        y = filtered_df[target_column]  # Target column (incident)\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        # Set default parameters if none are provided\n",
    "        if params is None:\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'binary_logloss',\n",
    "                'num_leaves': 31,\n",
    "                'learning_rate': 0.05,\n",
    "                'feature_fraction': 0.9,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'verbose': 0\n",
    "            }\n",
    "\n",
    "        # Create LightGBM dataset objects for training and testing\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "        # Train the LightGBM model\n",
    "        bst = lgb.train(params, train_data, num_round, valid_sets=[test_data], early_stopping_rounds=10)\n",
    "\n",
    "        # Predict probabilities for the positive class (incident = 1)\n",
    "        y_pred_proba = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "        # Add predicted probabilities as a new column in the filtered DataFrame\n",
    "        filtered_df['predicted_proba'] = y_pred_proba\n",
    "\n",
    "        # Plot feature importance\n",
    "        lgb.plot_importance(bst, figsize=(10, 8), importance_type='split')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "        return bst, filtered_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model training: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178af8a-5f33-403a-817f-6dbe6775b673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9a23e-2646-45fa-b772-34283844f8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3044217c",
   "metadata": {},
   "source": [
    "### Stage 7 : Evaluation & Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e483cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e10eee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
