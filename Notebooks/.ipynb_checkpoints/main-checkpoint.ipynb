{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16e2ae46-afbd-42c4-8f38-3cdcb83fa284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/dashtban/Ticker-Test/')\n",
    "\n",
    "import yaml\n",
    "# Read configuration from YAML file\n",
    "with open('config.yml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "from src.utils import *\n",
    "from src.enrich import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set low_memory to False\n",
    "pd.set_option('mode.use_inf_as_na', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0510c5b-b15e-4574-9574-cabdc05ad846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load proccesed files from R-studio \n",
    "df = pd.read_csv(config['data']['rdata_complete'])\n",
    "# Remove the  prefix\n",
    "df.columns = [col.replace('positions.', '') for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd058b7-687e-4fb3-bcd3-65f2970a3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 : Data Investigation\n",
    "\n",
    "# Column Characteristics before removing and cleansing\n",
    "column_statistics(df,config['path']['report']+'column_statistics_original_data.html')\n",
    "\n",
    "# Summary stats\n",
    "summary_html(df,config['path']['report']+'summary_statistics_numerical_values.html')\n",
    "\n",
    "# Generate maps base on geospatial data\n",
    "generate_maps(df,config['reports']['maps'])\n",
    "\n",
    "# Select randomly some rows\n",
    "select_random_rows_without_missing_values(df).to_csv(config['path']['report']+'temp.csv', index=False) \n",
    "# See some random rows with missing values \n",
    "select_rows_with_missing_values(df,config['path']['report']+'rows_with_missing_values.html')\n",
    "\n",
    "# generate_correlation_table\n",
    "generate_correlation_table(df,config['path']['report']+'summary_numerical_original_cohort.html')\n",
    "generate_correlation_heatmap(df,config['path']['report']+'df_original_cohort_corr.jpg')\n",
    "\n",
    "\n",
    "# Create missing indicator\n",
    "df = add_missing_indicator_column(df,threshold=.01,colname = 'missing')\n",
    "\n",
    "# Save the refined data \n",
    "df.to_pickle(config['path']['data']+'df_refined.pkl')\n",
    "df = pd.read_pickle(config['path']['data']+'df_refined.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ebd8b00-2df4-4186-b0c2-6ba6d12281f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wq/qwr2827d25g4h82c4hdh7mqw0000gn/T/ipykernel_98726/670893928.py:2: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(config['data']['rdata_scanned'])\n"
     ]
    }
   ],
   "source": [
    "# Load Scanned files from R-studio \n",
    "df = pd.read_csv(config['data']['rdata_scanned'])\n",
    "# Remove the  prefix\n",
    "df.columns = [col.replace('positions.', '') for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1a4e842-6b6d-4eff-ad91-84d4519cdc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba850cb4-67c2-4752-b5ad-7c2d332e9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Satge 2: Enrichment\n",
    "df = enrichment_pipeline(df)\n",
    "df.to_pickle(config['data']['refined_enriched'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e4eb4-3fce-4bfe-8fe9-f374c857c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_correlation_table\n",
    "df = pd.read_pickle(config['data']['refined_enriched'])\n",
    "\n",
    "# Generrate the corr plots\n",
    "generate_correlation_heatmap(df,config['path']['report']+'df_enriched_cohort_corr.jpg')\n",
    "sel_cols = ['duration','total_distance','average_speed', 'duration_minutes',\n",
    "            'direct_distance', 'gforce','Signal_Strength', 'Wavelet_XYZ_1', \n",
    "            'Wavelet_XYZ_2', 'Wavelet_XYZ_3','Wavelet_LonLat_1', 'Wavelet_LonLat_2',\n",
    "            'Wavelet_LonLat_3','PCA_gforce_1',\"PCA_speedMph_1\", 'Distinct_formOfWay_Count','incident']\n",
    "generate_correlation_heatmap(df[sel_cols],config['path']['report']+'df_enriched_cohort_corr.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05a7279e-f628-4a93-a809-15ee3acb0b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rdata_complete': './Data/Rdata/processed_data_complete.csv',\n",
       " 'rdata_events': './Data/Rdata/processed_data_with_events.csv',\n",
       " 'rdata_noevent': './Data/Rdata/processed_data_no_events.csv',\n",
       " 'rdata_scanned': '/Users/dashtban/Ticker-Test/Data/Rdata/processed_data_scanned.csv',\n",
       " 'refined': './Data/df_refined.pkl',\n",
       " 'refined_enriched': './Data/df_refined_enriched.pkl'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Satge 3. Modeling\n",
    "\n",
    "# Algorithm 1.\n",
    "# 1- Find the distance between all data points to the observed incidents\n",
    "# 2- Find the farthest one and select as much as the observed one to create a fully balanced one\n",
    "# 3- Train a model to distiguish between incidents and non-incidents\n",
    "# 4- Update the index pool with probility produced by the Trainer, (assing as incident if the prob>70%)\n",
    "# 5- with the updated lables, go to step 2\n",
    "# 6- continue until there is no data point remained\n",
    "# 7- Identify the highest risk journeys by evaluating the number of inciednts predicted per journey and assing severity index for each journey\n",
    "# 8- train another clasifer and see how well the true incidents could be detected,\n",
    "# 9- refine the procedure based on finding in step 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f0d43-5203-4551-8ad3-b00ae0e250c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distant_points(df, label_column='incident', num_points=12000):\n",
    "    \"\"\"\n",
    "    Finds the data points with the highest distance between instances with label 0 and label 1.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        label_column (str): The column name representing the label.\n",
    "        num_points (int): The number of data points to select.\n",
    "        \n",
    "    Returns:\n",
    "        high_distance_points_df (DataFrame): DataFrame with all columns and an additional column indicating highest distance points.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert columns to numeric, converting non-numeric values to NaN\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        # Drop rows with missing values\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Select data points for label 0\n",
    "        label_0 = df[df[label_column] == 0].values\n",
    "        \n",
    "        # Select data points for label 1\n",
    "        label_1 = df[df[label_column] == 1].values\n",
    "        \n",
    "        # Compute distances between instances with label 0 and label 1\n",
    "        if len(label_0) == 0 or len(label_1) == 0:\n",
    "            raise ValueError(\"No data points with label 0 or label 1 found.\")\n",
    "        \n",
    "        distances = np.sqrt(((label_0[:, np.newaxis] - label_1) ** 2).sum(axis=2))\n",
    "        \n",
    "        # Get indices of top num_points data points with highest distances\n",
    "        top_indices = np.unravel_index(np.argsort(distances.ravel())[-num_points:], distances.shape)\n",
    "        \n",
    "        # Create DataFrame with all columns\n",
    "        high_distance_points_df = df.copy()\n",
    "        \n",
    "        # Add additional column indicating highest distance points\n",
    "        high_distance_points_df['Is_Highest_Distance'] = 0\n",
    "        \n",
    "        # Set value 1 for highest distance points\n",
    "        high_distance_points_df.iloc[top_indices[0], high_distance_points_df.columns.get_loc(label_column)] = 1\n",
    "        \n",
    "        return high_distance_points_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Select as much as the observed samples\n",
    "num_samples = df[df['incident'] == 1].shape[0]\n",
    "\n",
    "# Select features for computing similarity\n",
    "sel_cols = ['duration','total_distance','average_speed', 'duration_minutes',\n",
    "            'direct_distance', 'gforce','Signal_Strength', 'Wavelet_XYZ_1', \n",
    "            'Wavelet_XYZ_2', 'Wavelet_XYZ_3','Wavelet_LonLat_1', 'Wavelet_LonLat_2',\n",
    "            'Wavelet_LonLat_3','PCA_gforce_1',\"PCA_speedMph_1\", 'Distinct_formOfWay_Count','incident']\n",
    "\n",
    "# Select top 1 row per UID\n",
    "df_1 = df.groupby('UID').head(1)\n",
    "df2 = df_1.sample(n=1000)\n",
    "\n",
    "# Call the function\n",
    "d_df = find_distant_points(df2, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07da49-38cd-4919-b4f7-b6ea5a8ec381",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train a LihgtGBM\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_lightgbm_model(df, target_column='incident', test_size=0.2, random_state=42, params=None, num_round=100, plot_file='feature_importance.png'):\n",
    "    try:\n",
    "        # Filter DataFrame based on the target column\n",
    "        filtered_df = df[df[target_column] == 1]\n",
    "\n",
    "        # Assuming you have features and target\n",
    "        X = filtered_df.drop(target_column, axis=1)  # Features\n",
    "        y = filtered_df[target_column]  # Target\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        # Set default parameters if not provided\n",
    "        if params is None:\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'binary_logloss',\n",
    "                'num_leaves': 31,\n",
    "                'learning_rate': 0.05,\n",
    "                'feature_fraction': 0.9,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'verbose': 0\n",
    "            }\n",
    "\n",
    "        # Create dataset for LightGBM\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "        # Train LightGBM model\n",
    "        bst = lgb.train(params, train_data, num_round, valid_sets=[test_data], early_stopping_rounds=10)\n",
    "\n",
    "        # Predict probabilities for the target class\n",
    "        y_pred_proba = bst.predict(X_test)\n",
    "\n",
    "        # Add predicted probabilities as a new feature in the DataFrame\n",
    "        filtered_df['predicted_proba'] = y_pred_proba\n",
    "\n",
    "        # Plot feature importance\n",
    "        lgb.plot_importance(bst, figsize=(10, 8))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "        return bst, filtered_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178af8a-5f33-403a-817f-6dbe6775b673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9a23e-2646-45fa-b772-34283844f8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
